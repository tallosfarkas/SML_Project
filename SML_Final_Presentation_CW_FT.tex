\documentclass[aspectratio=169]{beamer}\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlsng}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hldef}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usetheme{Madrid}
\usecolortheme{beaver}

\title[SML Project]{Predicting S\&P 500 Direction with Ensemble Methods}
\author{Christian Weißmeier \and Farkas Tallos}
\institute[WU Wien]{Statistical and Machine Learning (2025/26)}
\date{\today}
\date{18.11.2025}

% LaTeX packages for tables and graphics
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}

% Beamer options for cleaner look
\setbeamertemplate{navigation symbols}{}
\definecolor{lightgray}{rgb}{0.95, 0.95, 0.95}
\setbeamercolor{block body}{bg=lightgray}
\setbeamerfont{frametitle}{size=\large}
\setbeamerfont{block title}{size=\small}
\setbeamerfont{normal text}{size=\small}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% --- Title Slide ---
\begin{frame}
  \titlepage
\end{frame}

% --- Agenda ---
\begin{frame}
  \frametitle{Agenda}
  \tableofcontents
\end{frame}

%-------------------------------------------------------
% section
\section{Introduction and Data}
\begin{frame}{Motivation}
\center{
\begin{quote}
“Stock returns are predictable, but not by much.”\\[3mm]
\hfill \footnotesize — John H. Cochrane, \textit{Asset Pricing (2005)}
\end{quote}
}
\begin{itemize}
  \item Forecasting stock market direction is one of the most classical and challenging tasks in finance.
  \item Weak predictability can matter for portfolio allocation and risk management. \\

  \vspace{0.3cm}

  \textbf{Our Set-Up:}
  \vspace{0.1cm}
  \item We focus on predicting whether the S\&P 500 index goes \textbf{Up} or \textbf{Down} next month.
  \item We evaluate statistical learning methods in a time-series context.
  \item Introduce the role of regularization and nonlinear models.
  \item Compare linear vs. nonlinear classification models (Elastic Net vs. Random Forest vs. GBM).
\end{itemize}
\end{frame}

%-------------------------------------------------------
\begin{frame}{Data Overview}
\begin{itemize}
  \item We created our own monthly dataset from S\&P 500, FRED, and FRBSF data banks (1990–today).
  \item \textbf{Target:} Monthly S\&P 500 market direction (\textbf{UP} or \textbf{DOWN}) in \(t + 1\).
  \item \textbf{Predictors:}
  \begin{enumerate}
    \item \textbf{Market data:} Lagged S\&P 500 returns (up to five months) and trading volume changes.
    \item \textbf{Macroeconomic indicators:} CPI, Federal Funds Rate, NBER recession dummy.
    \item \textbf{Volatility:} Lagged changes in the VIX index.
    \item \textbf{Sentiment:} Daily News Sentiment Index.
  \end{enumerate}
  \item All features are lagged to avoid look-ahead bias.
\end{itemize}
\end{frame}


% section methdology
\section{Methodology}
\begin{frame}{Exploratory Analysis: Feature Relationships}

\begin{columns}[T, totalwidth=\textwidth]

  % Left: Plot
  \begin{column}{0.50\textwidth}
    \includegraphics[width=0.9\linewidth]{pair_plot.png} \\
    {\footnotesize Pairwise correlations and marginal distributions.}
  \end{column}

  % Right: Key insights
  \begin{column}{0.50\textwidth}
    \footnotesize
    \textbf{Why these predictors may matter:}
    \begin{itemize}
        \item \textbf{Macro indicators} (CPI, Fed Funds Rate, NBER) affect discount rates and expected returns.
        \item \textbf{Lagged returns} reflect momentum and reversal effects.
        \item \textbf{Volatility} (VIX) captures uncertainty and risk sentiment.
        \item \textbf{Sentiment} (DNSI) reflects investor expectations and behavioral biases.
    \end{itemize}

    \vspace{0.5em}
    \textbf{Empirical observation:}
    \begin{itemize}
        \item Weak linear correlations $\Rightarrow$ low-signal, non-linear problem.
        \item Several predictors show significant \textbf{ multicollinearity}.
        \item Motivates using \textbf{Elastic Net}, \textbf{Random Forests}, and \textbf{Gradient Boosting}.
    \end{itemize}
  \end{column}

\end{columns}

\end{frame}
%-------------------------------------------------------
\begin{frame}{Stationarity Considerations}
\begin{itemize}
  \item Most macro-financial time series are \textbf{non-stationary} in levels.
  \item We therefore use:
  \begin{itemize}
    \item \textbf{Lagged returns} instead of prices.
    \item \textbf{Changes} in VIX and sentiment rather than levels.
    \item \textbf{Changes in macro variables} to preserve temporal causality.
  \end{itemize}
  \item This transformation makes features approximately stationary, ensuring:
  \begin{itemize}
    \item Stable model coefficients over time.
    \item Valid cross-validation across time periods.
  \end{itemize}
\end{itemize}
\end{frame}

%-------------------------------------------------------
\begin{frame}{Hyperparameter Tuning: Time-Series Cross-Validation}
\footnotesize

\begin{itemize}
  \item[] \textbf{Why not standard cross-validation?}
  \begin{itemize}
    \item Random $k$-fold CV assumes i.i.d.\ data.
    \item Time-series data exhibit autocorrelation $\Rightarrow$ temporal dependence.
    \item Random shuffling lets the model see the future $\Rightarrow$ data leakage and over-optimism.
  \end{itemize}

  \vspace{0.4em}
  \item[] \textbf{Our method: Rolling-window time-series CV}
  \begin{itemize}
    \item Construct a hyperparameter grid:
      \begin{itemize}
        \item Elastic Net: $\alpha$ and $\lambda$ grid.
        \item Random Forest: \texttt{mtry}, \texttt{min.node.size}, \texttt{max.depth}, \texttt{sample.fraction}.
      \end{itemize}
    \item Use a chronological window structure:
      \begin{itemize}
        \item Initial training window: 60 months (5 years).
        \item Validation horizon: 12 months (1 year).
        \item Fixed-length rolling window moving forward in time.
      \end{itemize}
    \item For each fold:
      \begin{itemize}
        \item Fit model only on past data and predict the next 12 months.
        \item Record accuracy and AUC on the validation slice.
      \end{itemize}
  \end{itemize}
  \item[] \textbf{Result:}
  \begin{itemize}
    \item Select the hyperparameters achieving the \textbf{highest mean AUC across all rolling folds}.
    \item Prevents look-ahead bias and mimics real-time forecasting.
  \end{itemize}
\end{itemize}

\end{frame}
%-------------------------------------------------------

\begin{frame}{Class Imbalance, Threshold Adjustment \& AUC}

\begin{columns}[T, totalwidth=\textwidth]
  % Left: Plot
  \begin{column}{0.45\textwidth}
    \begin{center}
      \includegraphics[width=0.75\linewidth]{imbalance_plot.png}\\[0.3em]
      \footnotesize{Class imbalance in monthly S\&P 500 direction}
    \end{center}
  \end{column}

  % Right: Logic + AUC explanation
  \begin{column}{0.55\textwidth}
    \footnotesize
    \begin{itemize}
      \item Dependent variable \texttt{UP\_DOWN} is imbalanced.
      \item A naive classifier that would always predict ``Up'' would achieve high accuracy.
      \item A simple threshold of \(0.5\) reduces the accuracy. \\

      \vspace{0.1em}

      $\Rightarrow$ \textbf{Accuracy alone is misleading.}
    \end{itemize}

    \textbf{Threshold adjustment (Youden's J):}
    \[
      t^\ast = \arg\max_{t} \big\{ \text{Sensitivity}(t) + \text{Specificity}(t) - 1 \big\}
    \]
    \vspace{-1.2em}
    \begin{itemize}
      \item We tune $t^\ast$ on the \textbf{training set} using the ROC curve.
      \item Then apply this fixed threshold to the \textbf{test set} for true OoS evaluation.
    \end{itemize}

    \textbf{AUC is robust here:}
    \begin{itemize}
      \item ROC/AUC depends on the \emph{ranking} of predicted probabilities, not on class proportions. \\
      $\Rightarrow$ \textbf{AUC is invariant to class imbalance} and a suitable metric for our setting.
    \end{itemize}
  \end{column}
\end{columns}

\end{frame}

%-------------------------------------------------------
% section model results
\section{Model Results}
\subsection{Elastic Net}
\begin{frame}{Elastic Net Logistic Regression}
\small
\textbf{Model Intuition:}
\begin{itemize}
  \item We model the probability of an \textbf{Up} move using a logistic function:
  \[
  P(Y_t = 1 \mid X_t) = \frac{1}{1 + e^{-(\beta_0 + X_t \beta)}}
  \]
  \item Coefficients \(\beta\) are maximum likelihood estimates under a regularization penalty to prevent overfitting and perform variable selection.
\end{itemize}

\vspace{0.3cm}

\textbf{Elastic Net Regularization:}
\[
\min_{\beta} \left[
  -\ell(\beta)
  + \lambda \left(
      (1 - \alpha)\frac{\|\beta\|_2^2}{2}
      + \alpha \|\beta\|_1
    \right)
\right]
\]

\begin{itemize}
  \item \(\ell(\beta)\): log-likelihood of the logistic model.
  \item \(\lambda\): overall penalty strength controlling coefficient shrinkage.
  \item \(\alpha\): mixes the two types of regularization:
  \begin{itemize}
    \item Ridge (L2): smooth shrinkage.
    \item Lasso (L1): sets some coefficients exactly to zero.
  \end{itemize}
\end{itemize}
\end{frame}


%-------------------------------------------------------
\begin{frame}{Elastic Net: Out-of-Sample Performance}

\small
\textbf{Predictive performance (test set):}
\footnotesize{
\begin{itemize}
  \item Accuracy: \textbf{64.3\%}
  \item AUC: \textbf{0.79} (strong probability ranking ability)
  \item Threshold tuned : \(t^*= 0.6743 \) \(\rightarrow\) The model avoids overly optimistic Up predictions
\end{itemize}
}


\small{\textbf{Confusion matrix:}}
\footnotesize{
\begin{center}
\begin{tabular}{c|cc}
              & Actual 0 & Actual 1 \\
\hline
Predicted 0   & 36       & 38       \\
Predicted 1   & 8        & 47       \\
\end{tabular}
\end{center}
}

\small{\textbf{Interpretation:}}
\footnotesize{
\begin{itemize}
  \item \textbf{Specificity} (correct Down predictions):
            $36/44 \approx \textbf{82\%}$
            $\Rightarrow$ we correctly flag most Down months.
  \item \textbf{Sensitivity} (correct Up predictions):
            $47/85 \approx \textbf{55\%}$
            $\Rightarrow$ we still capture the majority of Up months.
  \item The model trades a small loss in raw accuracy for a much better
        \textbf{balance} between detecting costly Down markets and
        still identifying Up markets, which is desirable in an
        asset-management context.
\end{itemize}
}
\end{frame}

%-------------------------------------------------------

\begin{frame}{Elastic Net: Selected Predictors and Probability Ranking}

\small
\begin{columns}[T, totalwidth=\textwidth]

  % Left: Coefficients
  \begin{column}{0.52\textwidth}
  \centering{
    \includegraphics[width=0.75\linewidth]{enet_coeffs.png}
}
    {\scriptsize
    \begin{itemize}
    \item  Coeff. represent changes in the \textbf{odds} of an ``Up'' month.
    \item  Elastic Net highlights variables with consistent signal.
    \item  Momentum is a major driver of Up probabilities.
    \item  Sentiment matters more during recession periods.
    \end{itemize}
    }

  \end{column}

  % Right: ROC Curve
  \begin{column}{0.5\textwidth}
  \centering{
    \includegraphics[width=0.78\linewidth]{roc_enet.png}
} \\
\vspace{1.75em}
    {\scriptsize
    \begin{itemize}
    \item \textbf{AUC = 0.79}: strong discriminative ability despite low-signal, noisy financial data.
    \item ROC curve well above diagonal $\Rightarrow$ model ranks Up vs.\ Down months effectively.
    \end{itemize}}
  \end{column}
\end{columns}

\end{frame}

%-------------------------------------------------------
\subsection{Random Forest}





\begin{frame}{Random Forest (RF)}
\footnotesize
\textbf{Model Intuition:}
\begin{itemize}
    \item A bagging-based ensemble method, often described as the \textit{“wisdom of the crowd”}.
    \item Builds many decision trees in parallel, each trained on a bootstrap sample of the data.
    \item At each split, only a random subset of features (\texttt{mtry}) is considered, which de-correlates the trees.
    \item For classification, trees vote and the forest outputs the majority vote or averaged probabilities.
\end{itemize}

\vspace{0.2cm}

\textbf{Fixed Hyperparameters:}
\begin{itemize}
    \item \textbf{num.trees = 1000:} Large forest ensures stable predictions.
    \item \textbf{splitrule = "gini":} The Gini impurity measures how mixed the classes are within a node. 
      At each split, the Random Forest chooses the feature and split point that \textit{most reduces} Gini impurity. 
\end{itemize}


\textbf{Hyperparameters (from Time-Series CV):}
\begin{itemize}
    \item \textbf{mtry = 39:} Number of features considered at each split. Controls tree correlation.
    \item \textbf{min.node.size = 7:} Minimum samples per terminal node. Larger value = more regularization.
    \item \textbf{max.depth = 20:} Limits tree depth and prevents overly complex trees.
    \item \textbf{sample.fraction = 0.6:} Fraction of data sampled per tree. Lower values increase diversity.
    \item \textbf{num.trees = 1000:} Large forest ensures stable predictions.
\end{itemize}
\end{frame}





\begin{frame}{Random Forest: Out-of-Sample Performance}

\small
\textbf{Predictive performance (test set):}
\footnotesize{
\begin{itemize}
  \item Accuracy: \textbf{71.3\%}
  \item AUC: \textbf{0.753} (strong non-linear discriminative ability)
  \item RF produces well-calibrated probability scores without threshold tuning
\end{itemize}
}

\small{\textbf{Confusion matrix:}}
\footnotesize{
\begin{center}
\begin{tabular}{c|cc}
              & Actual 0 & Actual 1 \\
\hline
Predicted 0   & 29       & 22       \\
Predicted 1   & 15       & 63       \\
\end{tabular}
\end{center}
}

\small{\textbf{Interpretation:}}
\footnotesize{
\begin{itemize}
  \item \textbf{Specificity} (correct Down predictions):
        $29/44 \approx \textbf{66\%}$.
        RF detects a meaningful fraction of negative weeks.
  \item \textbf{Sensitivity} (correct Up predictions):
        $63/85 \approx \textbf{74\%}$.
        RF identifies most positive-return weeks.
  \item \textbf{Overall}: RF captures important \textbf{nonlinear return patterns},
        outperforming Elastic Net in accuracy and maintaining a strong AUC.
\end{itemize}
}

\end{frame}

%-------------------------------------------------------

\begin{frame}{Random Forest: Minimal Depth, Interpretation, and AUC}

\small
\begin{columns}[T, totalwidth=\textwidth]

%---------------------------------------------------
% LEFT COLUMN — Plot + Explanation + Interpretation
%---------------------------------------------------
\begin{column}{0.5\textwidth}

\centering{
  \includegraphics[width=0.78\linewidth]{rf_minimal_depth.png}
}

{\footnotesize \textbf{Explanation and Interpretation of Minimal Depth}}
{\scriptsize
\begin{itemize}
  \item Min. depth measures how \textbf{early} a variable is used in the tree.
  \item \textbf{VIX\_change\_lag} is the dominant signal
  \item \textbf{Lagged returns} are meaningful momentum predictors.
  \item Overall: RF relies mainly on \textbf{volatility shocks} and
        \textbf{recent price dynamics}.
\end{itemize}
}

\end{column}

%---------------------------------------------------
% RIGHT COLUMN — ROC + Takeaway
%---------------------------------------------------

\begin{column}{0.45\textwidth}

\centering{
  \includegraphics[width=0.8\linewidth]{roc_rf.png}
}

\raggedright

\vspace{0.1cm}

\footnotesize{ \textbf{AUC}} \\
\scriptsize{
\begin{itemize}
  \item \textbf{AUC = 0.753}
        $\Rightarrow$ strong discriminative ability and reliable ranking
        of Up vs.\ Down weeks.
\end{itemize}
}


\vspace{0.1cm}

\footnotesize{ \textbf{Takeaway}} \\
\scriptsize{
\begin{itemize}
\item Random Forest effectively captures nonlinear interactions between
\textbf{volatility movements} and \textbf{short-term momentum}.
\end{itemize}
}

\end{column}

\end{columns}

\end{frame}



\subsection{Gradient Boosting Machine}
%-------------------------------------------------------
\begin{frame}{Gradient Boosting Machine (GBM)}
% --- FONT CHANGED ---
\footnotesize 
\textbf{Model Intuition:}
\begin{itemize}
    \item A powerful \textbf{boosting} ensemble method, as discussed in class.
     \item It builds trees \textbf{sequentially}, not in parallel like Random Forest .
     \item Each new, simple tree (a "weak learner") is trained on the (pseudo-)residuals of the previous trees' errors.
     \item It's a "slow" learner that "boosts" the signal over many iterations, fitting an additive model.
     \item We use \texttt{distribution = "bernoulli"} for binary classification, which optimizes the deviance (log-loss).
\end{itemize}

\vspace{0.3cm}

% --- [MODIFIED SECTION] ---
\textbf{Tuned Hyperparameters (from Time-Series CV):}
\begin{itemize}
     \item \texttt{shrinkage = 0.01} (The learning rate $\nu$): A small value provides \textbf{regularization}, forcing the model to learn "slowly".  This is ideal for noisy financial data, as it prevents overfitting and generally gives better results.
       D
    \item \texttt{n.trees = 300} (The number of iterations $M$): This is the optimal stopping point found by CV.  It is directly balanced with the small shrinkage; a slow learner ($\nu=0.01$) requires more iterations ($M=300$) to fit the signal.
        
    \item \texttt{interaction.depth = 1} (Tree size $J$): This was a key finding.  Our CV selected a "stump" (a tree with one split), which restricts the entire GBM to a purely \textbf{additive model}. This suggests the predictive signal in our data is best captured by main effects, not by complex interactions.
\end{itemize}
% --- [END OF MODIFIED SECTION] ---
\end{frame}

%-------------------------------------------------------
%-------------------------------------------------------
\begin{frame}{GBM: Out-of-Sample Performance and Importance}

\begin{columns}[T, totalwidth=\textwidth]

% Left Column: Performance
\begin{column}{0.5\textwidth}
% --- FONT CHANGED ---
\footnotesize 
\textbf{Predictive performance (test set):}
% --- FONT CHANGED ---
\scriptsize{
\begin{itemize}
    \item Accuracy: \textbf{0.698}
    \item AUC: \textbf{0.786}
    \item Log-Loss: \textbf{0.5449}
    \item Tuned Threshold: \textbf{0.6091} (via training ROC)
\end{itemize}
}

% --- FONT CHANGED ---
\footnotesize{\textbf{Confusion matrix:}}
% --- FONT CHANGED ---
\scriptsize{
\begin{center}
% --- This data is from your R output ---
\begin{tabular}{c|cc}
        	& Actual 0 & Actual 1 \\
\hline
Predicted 0 & 30 	 & 25 	\\
Predicted 1 & 14 	 & 60 	\\
\end{tabular}
\end{center}
}

% --- FONT CHANGED ---
\footnotesize{\textbf{Interpretation:}}
% --- FONT CHANGED ---
\scriptsize{
\begin{itemize}
    \item \textbf{Specificity}: $30 / (30 + 14) \approx \textbf{68\%}$
    \item \textbf{Sensitivity}: $60 / (60 + 25) \approx \textbf{71\%}$
    \item The additive GBM model shows a strong balance, with high sensitivity to "Up" markets and good specificity for "Down" markets.
\end{itemize}
}
\end{column}

% Right Column: Feature Importance Plot
\begin{column}{0.5\textwidth}
\centering{
    % --- This assumes you ran the R command to create the plot ---
    \includegraphics[width=0.8\textwidth]{gbm_var_imp_plot.png}
}

% --- FONT CHANGED ---
\footnotesize{\textbf{Feature Importance Interpretation:}}
% --- FONT CHANGED ---
\scriptsize{
\begin{itemize}
    \item The model is \textbf{dominated} by a single predictor: \texttt{VIX\_change\_lag} (\textbf{61.7\%} relative influence).
    \item The best-tuned model was additive (\texttt{interaction.depth = 1}), so it only finds main effects.
    \item Other macro (\texttt{CPI\_lag}) and momentum (\texttt{lag3\_return}, \texttt{lag1\_return}) features now provide a more meaningful corrective adjustment (6-7\% each).
\end{itemize}
}
\end{column}

\end{columns}
\end{frame}



\section{Trading Strategy}

\begin{frame}
\frametitle{Strategy Backtest}
\tiny % Set base font size to tiny

% --- Text Block ---
\begin{itemize}
    \item We can simulate a trading strategy to see if the statistical edge (AUC $>$ 0.75) translates into a practical, usable result.
    \item This simulation uses the unseen test set and assumes no transaction costs. At the start of each month $t$, use the model's prediction.
    \item If P(Up) $>$ Tuned Threshold $\Rightarrow$ \textbf{Go Long} S\&P 500 for the month. If P(Up) $<$ Tuned Threshold $\Rightarrow$ \textbf{Go Short} S\&P 500 for the month.
\end{itemize}

% --- Plot Block ---
\begin{center}
    % You can adjust the width as needed. 0.7 might be good.
    \includegraphics[width=0.7\linewidth]{appendix_backtest_plot.png} 
\end{center}

\end{frame}






\begin{frame}
\frametitle{Strategy Backtest}
\footnotesize

\textbf{Motivation:}
\begin{itemize}
    \item We can quantify the visual performance from the backtest plot using standard financial metrics.
    \item \textbf{Annualized Sharpe Ratio:} The key metric for risk-adjusted return (Return / Volatility). Higher is better.
    \item \textbf{Maximum Drawdown (MaxDD):} The largest peak-to-trough loss. Measures "pain" or tail risk; smaller is better.
\end{itemize}

\begin{center}
    % This is the table generated by the corrected R code
    \includegraphics[width=0.6\linewidth]{appendix_performance_table.png} 
\end{center}

\begin{itemize} \scriptsize
    \item The Random Forest and GBM models generated the highest risk-adjusted returns (Sharpe Ratios), both significantly outperforming the "Buy and Hold" strategy.
    \item Impressively, all model-based strategies had a \textbf{lower Maximum Drawdown} than "Buy and Hold," suggesting they were successful at mitigating major losses.
    \item This confirms the models provide not just higher returns, but \textbf{smarter, risk-controlled} returns.
\end{itemize}

\end{frame}







\section{Conclusion}

\begin{frame}
\frametitle{Final Model Selection}
\footnotesize

\begin{center}
    % Use the full table from your appendix
    \includegraphics[width=0.5\linewidth]{appendix_full_summary_table.png}
\end{center}

\textbf{Model Selection Discussion:}
\begin{itemize} \footnotesize
    \item We face a classic trade-off between pure \textbf{ranking ability} and \textbf{strategy profitability}.
    \item \textbf{Best Ranking Model (AUC):} The \textbf{Elastic Net} (AUC=0.790) is the most reliable model for pure probabilistic discrimination.
    \item \textbf{Best Strategy Model (Sharpe):} The \textbf{Random Forest} (Sharpe=1.696) generated the most profitable risk-adjusted returns in our backtest.
\end{itemize}


\textbf{Final Model Decision:}
\begin{itemize} \footnotesize
    \item For a pure probability model, the \textbf{Elastic Net} is the winner.
    \item For a pure \textbf{trading strategy}, the \textbf{Random Forest} is the winner.
    \item This key insight - that the non-linear patterns (RF) were more profitable than the linear ones (ENet) - is the main finding of our analysis.
\end{itemize}

\end{frame}







%-------------------------------------------------------
%-------------------------------------------------------
\begin{frame}{Final Model Comparison}
\frametitle{Final Model Comparison}

\begin{columns}[T, totalwidth=\textwidth]
 % Left: Combined ROC Plot
 \begin{column}{0.5\textwidth}
    \centering{
        % This is the plot you provided
        \includegraphics[width=\textwidth]{combined_roc_plot.png}
    }
    \captionof{figure}{\footnotesize Combined ROC curves on the hold-out test set.}
 \end{column}

 % Right: Summary Table
 \begin{column}{0.5\textwidth}
    \centering{
        % This is the table you provided
        \includegraphics[width=0.9\textwidth]{final_summary_table.png}
    }
    
    % --- [FONT CHANGED] --- % Add a little space
    \footnotesize\textbf{Final performance metrics on the hold-out test set}
    \begin{footnotesize} % Start even smaller font for the list
    \begin{itemize}
        \item All models performed significantly better than chance (all AUC greater than 0.75).
        \item The linear \textbf{Elastic Net} had the highest Test AUC (0.790), narrowly beating the GBM (0.786).
        \item The \textbf{Random Forest} had the highest Test Accuracy (0.713), but the lowest AUC.
        \item This suggests the regularized linear model was best at ranking probabilities, while the RF was best at classification after threshold tuning.
    \end{itemize}
    \end{footnotesize} % End smaller font
 \end{column}
\end{columns}

\end{frame}












%-------------------------------------------------------

\begin{frame}
\frametitle{Conclusion \& Final Insights}
\footnotesize

\textbf{What We Did (Summary):}
\begin{itemize} \scriptsize
    \item We built, tuned, and tested three models (GLM, Bagging, Boosting) to predict S\&P 500 direction.
    \item We used a robust \textbf{rolling-window CV} to respect the time-series data and avoid look-ahead bias.
    \item We addressed \textbf{class imbalance} by tuning with AUC and finding an optimal probability threshold.
\end{itemize}

\textbf{What We Found (The Key Insight):}
\begin{itemize} \scriptsize
    \item \textbf{Monthly market direction is predictable} (all models had Test AUC $>$ 0.75).
    \item The best linear model (Elastic Net) had the best ranking ability (AUC $=$ 0.790).
    \item The best non-linear model (Random Forest) generated the most profit (Sharpe = 1.696).
    \item \textbf{Insight:} This implies that while linear signals (momentum, VIX) are the most consistent predictors, the \textbf{non-linear interactions} (e.g., VIX + sentiment) captured by the Random Forest, while perhaps less frequent, lead to more explosive, profitable moves.
\end{itemize}

\textbf{Implications \& Limitations:}
\begin{itemize} \scriptsize
    \item A simple long/short strategy based on the Random Forest model's non-linear signals would have significantly outperformed a "Buy and Hold" strategy on a risk-adjusted basis.
    \item \textbf{Limitations:} This backtest is idealized. It does not include transaction costs or slippage, and it assumes the model's relationships will remain stable in the future.
\end{itemize}

\end{frame}




%-------------------------------------------------------
% THANK YOU SLIDE
%-------------------------------------------------------
\section*{}
\begin{frame}
\frametitle{} % No title on this slide

\vfill % Pushes the content to the vertical center
\begin{center}
    % You can make the "Thank you" text larger
    \Huge{\textbf{Thank you!}}
    
    \vspace{1cm} % Adds some space
    
    \Large{Questions \& Discussion}
\end{center}
\vfill % Balances the vertical centering

\end{frame}



%-------------------------------------------------------
% APPENDIX SLIDE
%-------------------------------------------------------
\begin{frame}
\frametitle{Appendix: Full Probabilistic Performance}
\footnotesize

\textbf{Motivation:}
\begin{itemize}
  \item Proper scoring rules like Brier Score and Log-Loss assess the \textit{quality of the probabilities} themselves, not just the final classification.
  \item \textbf{Log-Loss} (Deviance) heavily penalizes confident wrong answers (e.g., predicting 0.9 when the answer is 0). This is the metric our GBM optimized.
  \item \textbf{Brier Score} is the Mean Squared Error of the probability, rewarding well-calibrated forecasts.
\end{itemize}

\vspace{0.5cm}

\begin{center}
  \includegraphics[width=0.8\linewidth]{appendix_full_summary_table.png}
\end{center}

\vspace{0.5cm}
\textbf{Insight:}
\begin{itemize}
  \item The \textbf{GBM} model, which directly optimized for Log-Loss ("bernoulli" distribution), achieves the best Log-Loss and Brier Score on the test set.
  \item This suggests that while the Elastic Net was best at \textit{ranking} (AUC), the GBM produced the most \textit{accurate and well-calibrated probabilities}.
\end{itemize}
\end{frame}






\begin{frame}
\frametitle{Appendix: RF Interactions}
\footnotesize


\begin{itemize}  \tiny
    \item Our key finding was that the non-linear RF model (Sharpe=1.696) was more profitable than the linear Elastic Net (Sharpe=1.058).
    \item \textbf{Why?} The RF model captures \textit{interaction effects} that the additive ENet and GBM (J=1) models miss.
\end{itemize}

\begin{center}
    % This is the plot you provided
    \includegraphics[width=0.5\linewidth]{appendix_rf_interactions.png}
\end{center}


\begin{itemize} \tiny
    \item This plot shows the 30 most frequent interactions. The strongest interactions are between \textbf{momentum} variables (e.g., \texttt{lag1\_return:lag3\_return}) and between \textbf{volatility and momentum} (e.g., \texttt{VIX\_change\_lag:lag1\_return}).
    \item \textbf{Conclusion:} This proves the RF model is capturing complex, non-linear rules (e.g., the effect of momentum depends on the VIX). This non-linearity is the source of its superior trading performance.
\end{itemize}
\end{frame}







\begin{frame}
\frametitle{Appendix: GBM Partial Dependence Plots}
\footnotesize

\textbf{Motivation:}
\begin{itemize}
  \item As discussed in class, Partial Dependence Plots (PDPs) are used to interpret the model by showing the marginal effect of a feature on the prediction, holding other features constant.
  \item This shows \textit{how} our most important variables influence the probability of an "Up" month.
\end{itemize}

\begin{columns}[T, totalwidth=\textwidth]

% --- [MODIFIED COLUMN 1] ---
  \begin{column}{0.5\textwidth}
    \centering % <-- REPLACED \begin{center}
    \includegraphics[width=0.8\textwidth]{appendix_pdp_vix.png}
    % <-- REMOVED \end{center}
    
    \vspace{-0.3cm} % <-- Optional: Add small negative space to pull text up
    \tiny
    \textbf{Insight:} The probability of an "Up" month is highest when the VIX has a small \textit{negative} change (slight calming). A large VIX spike (large positive change) dramatically \textit{decreases} the probability of an "Up" month.
  \end{column}

% --- [MODIFIED COLUMN 2] ---
  \begin{column}{0.5\textwidth}
    \centering % <-- REPLACED \begin{center}
    \includegraphics[width=0.8\textwidth]{appendix_pdp_lag3.png}
    % <-- REMOVED \end{center}
    
    \vspace{-0.3cm} % <-- Optional: Add small negative space to pull text up
    \tiny
    \textbf{Insight:} The model found a momentum effect. The probability of an "Up" month increases as the 3-month lagged return increases from negative to positive, consistent with financial theory.
  \end{column}
\end{columns}

\end{frame}






\begin{frame}
\frametitle{Appendix: RF Importance (Minimal Depth vs. Gini)}
\footnotesize

\begin{itemize} \tiny
  \item \textbf{Gini Importance} (left) measures the average gain in node purity from splitting on a variable. It is fast but can be biased towards continuous or high-cardinality features.
  \item \textbf{Minimal Depth} (right, from presentation) measures how early a variable is used to split. Variables that split early are generally more important. This is often considered more robust.
\end{itemize}

\begin{columns}[T, totalwidth=\textwidth]
  \begin{column}{0.5\textwidth}
    \centering
    \includegraphics[width=0.8\linewidth]{appendix_gini_plot.png}
  \end{column}
  \begin{column}{0.5\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{rf_minimal_depth.png}
  \end{column}
\end{columns}


\begin{itemize} \tiny
  \item Both metrics agree on the top 2 predictors: \textbf{VIX\_change\_lag} and \textbf{lag1\_return}.
  \item This consistency gives us high confidence that these are the most powerful and reliable signals in our non-linear model.
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Appendix: Hyperparameter Tuning Grids (Elastic Net)}
\footnotesize % <-- Set base font size

\textbf{Elastic Net (GLM) Grid:}
\begin{itemize}
    \item \texttt{alpha (Mixing Parameter)}: \{0, 0.1, 0.2, ..., 0.9, 1.0\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item Tests the \textbf{entire spectrum} of regularization.
            \item \texttt{alpha = 0} is pure Ridge (good for multicollinearity).
            \item \texttt{alpha = 1} is pure Lasso (good for variable selection).
            \item In between is the Elastic Net, which balances both. Your optimal $\alpha=0.1$ suggests a model that is 90 percent Ridge and 10 percent Lasso.
        \end{itemize}
    
    \item \texttt{lambda (Penalty Strength)}: 40 log-spaced values from $10^{-4}$ to $10$.
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item A log-spaced grid is critical because the penalty's effect is not linear.
            \item This wide range tests models from a standard (un-penalized) GLM (low $\lambda$) up to a null (intercept-only) model (high $\lambda$).
        \end{itemize}
\end{itemize}
\textit{Total combinations tested per CV fold: 11 (alphas) $\times$ 40 (lambdas) = 440}

\vspace{0.5cm}
\textbf{Insight:}
\begin{itemize} \scriptsize
    \item The tuning process is thorough. It selected $\alpha=0.1$, confirming that our data benefits from a Ridge-dominant model to handle multicollinearity, while still using a small amount of Lasso for variable selection.
\end{itemize}

\end{frame}




\begin{frame}
\frametitle{Appendix: Hyperparameter Tuning Grids}
\tiny % <-- Set base font size to footnotesize

\textbf{Random Forest Grid:}
\begin{itemize}
    \item \texttt{mtry (vars. per split)}: \{3, 4, 6, 7, 11\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item Based on $p=11$ predictors. Grid extends from the defaults ($\sqrt{p} \approx 3$, $p/3 \approx 4$) up to $p=11$ to find the optimal level of tree de-correlation.
        \end{itemize}
    \item \texttt{min.node.size (leaf size)}: \{2, 5, 10, 20\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item Controls tree depth and regularization.
        \end{itemize}
    \item \texttt{max.depth}: \{5, 10, 15, 20, Unlimited (NA)\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item Another regularization control on tree complexity.
        \end{itemize}
    \item \texttt{sample.fraction}: \{0.6, 0.7, 0.8, 0.9\}
\end{itemize}
\textit{Total combinations tested via Time-Series CV: 400}

\vspace{0.4cm}

\textbf{Gradient Boosting (GBM) Grid:}
\begin{itemize}
    \item \texttt{n.trees ($M$)}: \{100, 200, 300, 400\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item The number of boosting iterations (weak learners).
        \end{itemize}
    \item \texttt{interaction.depth ($J$)}: \{1, 2, 3\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item Tests an \textbf{additive model ($J$=1)} against models with two-way ($J$=2) and three-way ($J$=3) interactions.
        \end{itemize}
    \item \texttt{shrinkage ($\nu$)}: \{0.01, 0.1\}
        \begin{itemize} \scriptsize % <-- Set nested list to scriptsize
            \item The learning rate. Tests a slow, regularized rate (0.01) vs. a faster, standard rate (0.1).
        \end{itemize}
\end{itemize}
\textit{Total combinations tested via Time-Series CV: 24}

\end{frame}




%-------------------------------------------------------
% BIBLIOGRAPHY / REFERENCES SLIDE
%-------------------------------------------------------
\begin{frame}
\frametitle{References}
\footnotesize

% The \bibitem command is used for each entry.
% The [1], [2], etc., is the label.
\begin{thebibliography}{99}

  \bibitem{cochrane}
  Cochrane, John H. (2005).
  \textit{Asset Pricing}.
  Princeton University Press.

  \bibitem{fred}
  Federal Reserve Economic Data (FRED).
  \textit{St. Louis Fed}.
  Retrieved 2025.

  \bibitem{sf_fed}
  Federal Reserve Bank of San Francisco.
  \textit{Daily News Sentiment Index}.
  Retrieved 2025.

  \bibitem{yahoo}
  Yahoo Finance.
  \textit{S\&P 500 (GSPC) \& CBOE VIX (VIX) Data}.
  Retrieved 2025.
  
  % --- [FIXED] Escaped all the '&' symbols ---
  \bibitem{glmnet}
  Friedman, J., Hastie, T., \& Tibshirani, R. (2010).
  Regularization Paths for Generalized Linear Models via Coordinate Descent.
  \textit{Journal of Statistical Software, 33}(1), 1-22.

  \bibitem{ranger}
  Wright, M. N. \& Ziegler, A. (2017).
  ranger: A Fast Implementation of Random Forests...
  \textit{Journal of Statistical Software, 77}(1), 1-17.
  
  \bibitem{gbm}
  Ridgeway, G. (2007).
  Generalized Boosted Models: A guide to the gbm package.
  
  \bibitem{isl}
  James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2021).
  \textit{An Introduction to Statistical Learning: with Applications in R} (Second Edition).
  Springer.

\end{thebibliography}

\end{frame}

\end{document}
