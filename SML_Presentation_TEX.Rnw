\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{beaver}

\title[SML Project]{Predicting S\&P 500 Direction with Ensemble Methods}
\author{Christian Wei√ümeier \and Farkas Tallos}
\institute[WU Wien]{Statistical and Machine Learning (2025/26)}
\date{\today}

% LaTeX packages for tables and graphics
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs} % For nice tables
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption} % For \captionof

% Beamer options for cleaner code display
\setbeamertemplate{navigation symbols}{}
\definecolor{lightgray}{rgb}{0.95, 0.95, 0.95}
\setbeamercolor{block body}{bg=lightgray}
\setbeamerfont{frametitle}{size=\large}
\setbeamerfont{block title}{size=\small}
\setbeamerfont{normal text}{size=\small}

\begin{document}

% --- Title Slide ---
\begin{frame}
  \titlepage
\end{frame}

% --- Agenda Slide ---
\begin{frame}
  \frametitle{Agenda}
  \tableofcontents
\end{frame}

% --- Section 1: Introduction ---
\section{1. Introduction \& Data}
\begin{frame}
  \frametitle{1. Introduction: Task \& Motivation}
  \begin{itemize}
    \item \textbf{Task:} We selected a binary classification task.
    \item \textbf{Application Context:} Predict the monthly direction (\textit{Up} or \textit{Down}) of the S\&P 500 index.
    \item \textbf{Motivation:}
    \begin{itemize}
        \item This is a classic, challenging problem in financial econometrics.
        \item We want to determine if publicly available data (market, macro, sentiment) contains a predictive signal for market direction.
        \item This project allows us to apply and compare the course's key supervised learning methods to a complex, real-world time-series problem.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{1. Data Sourcing \& Pre-processing}
  \framesubtitle{A Rich, High-Dimensional Time-Series Dataset}
  
  We gathered a dataset spanning over 70 years (1950 - Present) to ensure our models are robust across multiple economic regimes (e.g., high inflation, recessions).
  
  \begin{columns}[T]
    \begin{column}{.5\textwidth}
      \textbf{Data Sources:}
      \begin{itemize}
          \item \textbf{Market (Yahoo):} S\&P 500 Price/Volume.
          \item \textbf{Macro (FRED):} CPI, Fed Funds Rate, NBER Recession.
          \item \textbf{Sentiment (FRB):} Daily News Sentiment Index (DNSI).
          \item \textbf{Volatility (Yahoo):} VIX Index.
      \end{itemize}
    \end{column}
    \begin{column}{.5\textwidth}
      \textbf{Key Pre-processing Steps:}
      \begin{itemize}
          \item All data aggregated to a monthly frequency.
          \item Predictors are \textbf{lagged} to prevent lookahead bias.
          \item \textbf{Target (Y):} \texttt{UP\_DOWN} (factor: "Up", "Down").
      \end{itemize}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{1. Final Features}
  \framesubtitle{Market, Macro, Sentiment, and Interactions}
  
  Our final model dataset includes \textbf{16 predictors}.
  
  \begin{itemize}
      \item \textbf{Market Lags (5):} \texttt{lag1\_return}, \texttt{lag2\_return}, ..., \texttt{lag5\_return}
      \item \textbf{Volume (1):} \texttt{volume\_change\_lag}
      \item \textbf{Macro (3):} \texttt{CPI\_lag}, \texttt{FedFundsRate\_lag}, \texttt{NBER\_lag} (Recession binary)
      \item \textbf{Volatility (1):} \texttt{VIX\_change\_lag}
      \item \textbf{Sentiment (1):} \texttt{DNSI\_change\_lag}
      \item \textbf{Interactions (4):} We engineered interaction terms to capture more complex relationships (e.g., \texttt{DNSI\_VIX\_lag}, \texttt{VIX\_CPI\_lag}).
  \end{itemize}
  
  \vfill
  \pause
  \begin{block}{Dataset Size}
    After merging and lagging, our final dataset for modeling runs from \textbf{Jan 1990 to Oct 2025}, providing \textbf{427 monthly observations}.
  \end{block}
\end{frame}

% --- Section 2: EDA ---
\section{2. Exploratory Data Analysis}
\begin{frame}
  \frametitle{2. Exploratory Data Analysis (EDA)}
  \framesubtitle{Non-linearity is Likely}
  
  \begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth, height=0.7\textheight, keepaspectratio]{ggpairs_plot.png}
    \caption{EDA: Predictor Relationships by Market Direction (Subset)}
  \end{figure}
  
\end{frame}

\begin{frame}
  \frametitle{2. EDA: Key Takeaways}
  \begin{itemize}
    \item The \texttt{ggpairs} plot shows no simple, linear "silver bullet" predictor. The distributions (histograms) for "Up" and "Down" months overlap significantly.
    \item This suggests that simple linear models may struggle and that predictive power, if it exists, likely comes from \textbf{non-linear relationships} or \textbf{interactions} between features.
    \item We also observed high correlation between lagged returns (e.g., \texttt{lag1\_return}, \texttt{lag2\_return}), which motivates the use of models that can handle collinearity.
  \end{itemize}
\end{frame}

% --- Section 3: Methodology ---
\section{3. Methodology}
\begin{frame}
  \frametitle{3. Methodology: Model Selection}
  \framesubtitle{Comparing Linear, Bagging, and Boosting Methods}
  
  We selected three distinct, powerful methods covered in the course, as required:
  
  \begin{columns}[T]
    \begin{column}{.33\textwidth}
      \begin{block}{1. Elastic Net (Regularized GLM)}
        \texttt{glmnet}
        \begin{itemize}
          \item A regularized logistic regression.
          \item Combines $L_1$ (Lasso) and $L_2$ (Ridge) penalties.
          \item \textbf{Why:} Excellent for variable selection (Lasso) and handling our correlated predictors (Ridge).
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{.33\textwidth}
      \begin{block}{2. Random Forest (Bagging)}
        \texttt{ranger}
        \begin{itemize}
          \item Ensembles many de-correlated decision trees.
          \item A substantial modification of bagging.
          \item \textbf{Why:} Very robust, captures non-linearities, and is not prone to overfitting.
        \end{itemize}
      \end{block}
    \end{column}
    \begin{column}{.33\textwidth}
      \begin{block}{3. Gradient Boosting (GBM)}
        \texttt{gbm}
        \begin{itemize}
          \item Sequentially builds "weak" trees that correct prior errors.
          \item \textbf{Why:} Often provides top-tier accuracy and models complex interactions.
        \end{itemize}
      \end{block}
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}
  \frametitle{3. Methodology: Model Assessment Strategy}
  \framesubtitle{\textbf{This is the most critical methodological slide}}
  
  \begin{block}{The Problem: Time-Series Data}
    We cannot use standard $K$-fold cross-validation. It shuffles data randomly, "peeking into the future" and violating the temporal order. This would lead to invalid, overly optimistic results.
  \end{block}
  
  \pause
  
  \begin{block}{Our Solution: A Two-Level Chronological Split}
  \begin{itemize}
      \item \textbf{Level 1: Train/Test Split (for Assessment)}
      \begin{itemize}
          \item We split our 418 observations \textbf{chronologically} (70/30).
          \item \textbf{Training Set (n=292):} 1990-2014. Used for all model tuning.
          \item \textbf{Test Set (n=126):} 2015-2025. Held out completely. Used only once at the end for final, unbiased model assessment.
      \end{itemize}
      \pause
      \item \textbf{Level 2: Rolling-Window CV (for Tuning)}
      \begin{itemize}
          \item To tune hyperparameters (e.g., $\lambda$, \texttt{mtry}), we perform a \textbf{rolling-window validation} inside the 70\% training set.
          \item This simulates real-world use: we train on past data to predict the immediate future.
          \item \textit{(See Appendix for implementation code)}
      \end{itemize}
  \end{itemize}
  \end{block}
\end{frame}

% --- Section 4: Results ---
\section{4. Results}


\begin{frame}
  \frametitle{4. Results: Hyperparameter Tuning Elastic Net}
  \framesubtitle{Best Parameters from Rolling-Window CV (Optimizing Log-Loss)}
  
  \begin{block}{1. Tuned Elastic Net}
  \centering
  \captionof{table}{Top 5 Elastic Net tuning results (lowest log-loss)}
  \resizebox{0.7\textwidth}{!}{
  \begin{tabular}{rrrr}
  \toprule
  alpha & lambda & mean\_LogLoss & mean\_AUC \\
  \midrule
  1.00 & 0.0215 & 0.584 & 0.700 \\
  0.50 & 0.0464 & 0.584 & 0.735 \\
  0.25 & 0.1000 & 0.586 & 0.730 \\
  0.75 & 0.0464 & 0.587 & 0.714 \\
  0.25 & 0.0464 & 0.589 & 0.696 \\
  \bottomrule
  \end{tabular}
  }
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{4. Results: Hyperparameter Tuning Random Forest}
  \framesubtitle{Best Parameters from Rolling-Window CV (Optimizing Log-Loss)}

  
  \begin{block}{2. Tuned Random Forest}
  \centering
  \captionof{table}{Top 5 Random Forest tuning results (lowest log-loss)}
  \resizebox{0.7\textwidth}{!}{
  \begin{tabular}{rrrrr}
  \toprule
  mtry & min.node.size & sample.fraction & mean\_LogLoss & mean\_AUC \\
  \midrule
  2 & 1 & 0.8 & 0.633 & 0.664 \\
  2 & 5 & 0.6 & 0.633 & 0.674 \\
  2 & 10 & 0.6 & 0.637 & 0.691 \\
  2 & 10 & 0.8 & 0.642 & 0.693 \\
  2 & 1 & 0.6 & 0.644 & 0.657 \\
  \bottomrule
  \end{tabular}
  }
  \end{block}

\end{frame}



\begin{frame}
  \frametitle{4. Results: Hyperparameter Tuning GBM}
  \framesubtitle{Best Parameters from Rolling-Window CV (Optimizing Log-Loss)}
  
  
  \begin{block}{3. Tuned Gradient Boosting (GBM)}
  \centering
  \captionof{table}{Top 5 GBM tuning results (lowest log-loss)}
  \resizebox{0.7\textwidth}{!}{
  \begin{tabular}{rrrrr}
  \toprule
  n.trees & interaction.depth & shrinkage & mean\_LogLoss & mean\_AUC \\
  \midrule
  300 & 2 & 0.01 & 0.612 & 0.698 \\
  200 & 2 & 0.01 & 0.613 & 0.707 \\
  300 & 1 & 0.01 & 0.613 & 0.685 \\
  200 & 3 & 0.01 & 0.614 & 0.708 \\
  200 & 1 & 0.01 & 0.615 & 0.711 \\
  \bottomrule
  \end{tabular}
  }
  \end{block}
\end{frame}



\begin{frame}
  \frametitle{4. Results: Final Model Assessment 1/2}
  \framesubtitle{Comparing Performance on the Hold-Out Test Set (2015-2025)}
  
  \begin{center}
  \captionof{table}{Final Model Performance on Hold-Out Test Set}
  \begin{tabular}{lrrr}
  \toprule
  Model & Test\_AUC & Test\_Accuracy & Test\_LogLoss\\
  \midrule
  Elastic Net (Tuned) & 0.7880 & 0.7670 & 0.5143\\
  Gradient Boosting (Tuned) & 0.7820 & 0.7210 & 0.5381\\
  Random Forest (Tuned) & 0.7710 & 0.7130 & 0.5518\\
  \bottomrule
  \end{tabular}
  \end{center}
  
\end{frame}


\begin{frame}
  \frametitle{4. Results: Final Model Assessment 2/2}
  \framesubtitle{Comparing Performance on the Hold-Out Test Set (2015-2025)}

  \begin{figure}
    \centering
    % You must save your combined ROC plot as 'combined_roc_plot.png'
    \includegraphics[width=0.53\textwidth]{combined_roc_plot.png}
    \caption{Final Model ROC Comparison (Test Set)}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{4. Results: Insights from Best Model}
  \framesubtitle{Coefficients from the Tuned Elastic Net}
  
  The \textbf{Elastic Net} was our best-performing model (Test AUC: 0.788). As a regularized GLM, its coefficients show which features were most important.
  
  Our tuned model used $\alpha=1.0$ (Lasso), which performed variable selection, setting 10 of 15 predictors to zero.
  
  \begin{columns}[T] % Use [T] for top-alignment
    
    % --- First Column (Table 1) ---
    \begin{column}{.35\textwidth}
      \begin{block}{Final Model Coefficients (1/2)}
        \centering
        \resizebox{0.9\linewidth}{!}{ % Resize table to fit column
        \begin{tabular}{lr}
        \toprule
        Predictor & Coefficient \\
        \midrule
        (Intercept)       & 0.5613 \\
        \texttt{CPI\_lag}         & .      \\
        \texttt{FedFundsRate\_lag} & .      \\
        \texttt{NBER\_lag}        & -0.5159 \\
        \texttt{lag1\_return}     & -0.5606 \\
        \texttt{lag2\_return}     & .      \\
        \texttt{lag3\_return}     & 1.4206 \\
        \texttt{lag4\_return}     & .      \\
        \bottomrule
        \end{tabular}
        }
      \end{block}
    \end{column}
    
    % --- Second Column (Table 2) ---
    \begin{column}{.35\textwidth}
      \begin{block}{Final Model Coefficients (2/2)}
        \centering
        \resizebox{0.9\linewidth}{!}{ % Resize table to fit column
        \begin{tabular}{lr}
        \toprule
        Predictor & Coefficient \\
        \midrule
        \texttt{lag5\_return}       & 1.3327 \\
        \texttt{volume\_change\_lag} & .      \\
        \texttt{VIX\_change\_lag}  & -5.5891 \\
        \texttt{DNSI\_change\_lag}  & .      \\
        \texttt{DNSI\_VIX\_lag}     & .      \\
        \texttt{DNSI\_FedFunds\_lag} & .      \\
        \texttt{VIX\_CPI\_lag}      & .      \\
        \texttt{DNSI\_NBER\_lag}    & .      \\
        \bottomrule
        \end{tabular}
        }
      \end{block}
    \end{column}
    
  \end{columns}
\end{frame}

% --- Section 5: Conclusion ---
\section{5. Conclusion}
\begin{frame}
  \frametitle{5. Conclusion \& Discussion}
  
  \begin{itemize}
    \item \textbf{Correctness of Results:}
    \begin{itemize}
        \item We successfully implemented a \textbf{time-series-aware} validation pipeline to select and assess 3 models from the course.
        \item The rolling-window tuning was essential for correctly handling the data's temporal structure and avoiding lookahead bias.
    \end{itemize}
    \item \textbf{Final Performance:}
    \begin{itemize}
        \item The \textbf{Tuned Elastic Net} was the best model, achieving a Test AUC of \textbf{0.788}.
        \item This performance is strong and clearly better than random guessing (AUC = 0.5), suggesting a predictive signal exists.
    \end{itemize}
    \item \textbf{Key Insights:}
    \begin{itemize}
        \item The model selected only 5 predictors.
        \item The most important predictor was \texttt{VIX\_change\_lag} with a large negative coefficient, indicating that a recent spike in volatility is a strong predictor of a 'Down' month.
        \item Recent momentum is complex: \texttt{lag1\_return} is negative, but \texttt{lag3\_return} and \texttt{lag5\_return} are positive.
        \item Being in a recession (\texttt{NBER\_lag}) is a strong negative predictor, as expected.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
    \Huge Thank You
    \vfill
    \Large Questions?
  \end{center}
\end{frame}






\appendix
\begin{frame}[fragile]
  \frametitle{Appendix: Model Coefficients \& Tuning Code}
  
  \begin{columns}[T] % Create columns for the first two blocks
    
    \begin{column}{.5\textwidth}
      \begin{block}{Tuned Elastic Net Coefficients}
      \tiny % Use smallest font
      \begin{verbatim}
  16 x 1 sparse Matrix of class "dgCMatrix"
  (Intercept)        0.5613457
  CPI_lag            .        
  FedFundsRate_lag   .        
  NBER_lag          -0.5158768
  lag1_return       -0.5606195
  lag2_return        .        
  lag3_return        1.4206226
  lag4_return        .        
  lag5_return        1.3326377
  volume_change_lag  .        
  VIX_change_lag    -5.5890975
  DNSI_change_lag    .        
  DNSI_VIX_lag       .        
  DNSI_FedFunds_lag  .        
  VIX_CPI_lag        .        
  DNSI_NBER_lag      . 
      \end{verbatim}
      \end{block}
    \end{column}
    
    \begin{column}{.5\textwidth}
      \begin{block}{Tuned GBM Feature Importance}
      \tiny % Use smallest font
      \begin{verbatim}
                             var   rel.inf
  VIX_change_lag    VIX_change_lag 29.1156079
  VIX_CPI_lag          VIX_CPI_lag 14.6891879
  lag3_return          lag3_return  8.8438007
  lag1_return          lag1_return  8.6205320
  CPI_lag                  CPI_lag  7.4618803
  lag2_return          lag2_return  7.1084220
  lag5_return          lag5_return  6.7386632
  FedFundsRate_lag  FedFundsRate_lag  4.6935959
  DNSI_change_lag  DNSI_change_lag  3.6435704
  lag4_return          lag4_return  2.9619767
  volume_change_lag volume_change_lag  1.6682523
  DNSI_VIX_lag      DNSI_VIX_lag  1.4948200
  DNSI_FedFunds_lag DNSI_FedFunds_lag  1.2914756
  NBER_lag                NBER_lag  1.0454376
  DNSI_NBER_lag      DNSI_NBER_lag  0.6227775
      \end{verbatim}
      \end{block}
    \end{column}
    
  \end{columns}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Appendix: Rolling-Window Tuning Loop (Random Forest)}
  
  \begin{columns}[T] % Top-aligned columns
    
    % --- First Column ---
    \begin{column}{.5\textwidth}
      \begin{block}{RF Tuning Loop (Part 1)}
      \tiny % Use smallest font
      \begin{verbatim}
# --- Rolling-window tuning ---
n_train <- nrow(train_df)
window_size <- floor(0.7 * n_train)
horizon <- 12
results <- data.frame()

set.seed(123)
for (i in seq(window_size, n_train - horizon, 
             by = horizon)) {
  train_window <- train_df[1:i, ]
  test_window  <- train_df[(i + 1):(i + horizon), ]
  
  if (nrow(test_window) == 0 || 
      length(unique(test_window$Y)) < 2) next
  
  for (j in 1:nrow(param_grid)) {
    p <- param_grid[j, ]
    
    rf_model <- ranger(
      Y ~ .,
      data = train_window,
      num.trees = 500,
      mtry = p$mtry,
      min.node.size = p$min.node.size,
      sample.fraction = p$sample.fraction,
    # (Continued in next column...)
      \end{verbatim}
      \end{block}
    \end{column}
    
    % --- Second Column ---
    \begin{column}{.5\textwidth}
      \begin{block}{RF Tuning Loop (Part 2)}
      \tiny % Use smallest font
      \begin{verbatim}
    # (...Continued from last column)
      probability = TRUE,
      seed = 123
    )
    preds <- predict(rf_model, 
      data = test_window)$predictions[, "1"]
    
    y_true <- as.numeric(as.character(
      test_window$Y
    ))
    
    logloss <- -mean(
      y_true * log(preds + eps) + 
      (1 - y_true) * log(1 - preds + eps)
    )
    
    results <- rbind(results, data.frame(
        mtry = p$mtry, 
        min.node.size = p$min.node.size,
        sample.fraction = p$sample.fraction, 
        LogLoss = logloss
      ))
  } # end inner loop
} # end outer loop
      \end{verbatim}
      \end{block}
    \end{column}
    
  \end{columns}
  
\end{frame}

\end{document}